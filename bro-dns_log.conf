input {
  beats {
    host => "172.16.100.123"
    port => 5044
  }
}

filter {
  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-dns" {
    csv {
      columns => ["ts","uid","id.orig_h","id.orig_p","id.resp_h","id.resp_p","proto","trans_id","rtt","query","qclass","qclass_name","qtype","qtype_name","rcode","rcode_name","AA","TC","RD","RA","Z","answers","TTLs","rejected"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }

    mutate {
      convert => { "id.orig_p" => "integer" }
      convert => { "id.resp_p" => "integer" }
      convert => { "trans_id" => "integer" }
      convert => { "rtt" => "float" }
      convert => { "TTLs" => "string" }
      convert => { "qclass" => "integer" }
      convert => { "qtype" => "integer" }
      convert => { "rcode" => "integer" }
      rename =>  { "id.orig_h" => "id_orig_host" }
      rename =>  { "id.orig_p" => "id_orig_port" }
      rename =>  { "id.resp_h" => "id_resp_host" }
      rename =>  { "id.resp_p" => "id_resp_port" }
    }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    manage_template => false
    sniffing => true
    index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
    document_type => "%{[@metadata][type]}"
  }
}
