input {
  beats {
    host => ""
    port => 5044
  }
}

filter {
  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-conn" {
    csv {
      columns => ["ts","uid","id.orig_h","id.orig_p","id.resp_h","id.resp_p","proto","service","duration","orig_bytes","resp_bytes","conn_state","local_orig","local_resp","missed_bytes","history","orig_pkts","orig_ip_bytes","resp_pkts","resp_ip_bytes","tunnel_parents"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }

    mutate {
      convert => { "id.orig_p" => "integer" }
      convert => { "id.resp_p" => "integer" }
      convert => { "orig_bytes" => "integer" }
      convert => { "duration" => "float" }
      convert => { "resp_bytes" => "integer" }
      convert => { "missed_bytes" => "integer" }
      convert => { "orig_pkts" => "integer" }
      convert => { "orig_ip_bytes" => "integer" }
      convert => { "resp_pkts" => "integer" }
      convert => { "resp_ip_bytes" => "integer" }
      rename =>  { "id.orig_h" => "id_orig_host" }
      rename =>  { "id.orig_p" => "id_orig_port" }
      rename =>  { "id.resp_h" => "id_resp_host" }
      rename =>  { "id.resp_p" => "id_resp_port" }
    }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    manage_template => false
    sniffing => true
    index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
    document_type => "%{[@metadata][type]}"
  }
}
